{
 "metadata": {
  "name": "",
  "signature": "sha256:75285b7396065f5b9e7d96dc67ebab35799b80a63cafe0add63589bcbf8bb4cd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create sentiment dictionary\n",
      "#classify tweet sentiment\n",
      "#scrape twitter user data and do a segmentation analysis on twitterhandles\n",
      "#build business plan around different user groups\n",
      "#possibly do a network graph?\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import re, math, collections, itertools\n",
      "import nltk, nltk.classify.util, nltk.metrics\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk.probability import FreqDist, ConditionalFreqDist\n",
      "import pylab\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['pylab']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentdf = pd.DataFrame(Sent_Dict, columns=['Words', 'Score'])\n",
      "sentdf['Score'].describe()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 142,
       "text": [
        "count     2477\n",
        "unique      11\n",
        "top         -2\n",
        "freq       966\n",
        "Name: Score, dtype: object"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentdf['Score'] = sentdf['Score'].apply(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#categorize words as very negative to very positive and add some movie-specific words\n",
      "NegTerms = sentdf[sentdf['Score']<0]\n",
      "PosTerms = sentdf[sentdf['Score']>0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#read saved scraped file\n",
      "myfile = 'CSVFILE HERE' #include your file here\n",
      "df = pd.DataFrame.from_csv(myfile, index_col=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all tweets authored by twitter handle\n",
      "\n",
      "#to build sentiment engine do feature extraction, naive bayesian classifier seems ot be the way to go\n",
      "print len(df)\n",
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7785\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 172,
       "text": [
        "Index([u'created_at', u'tweetid', u'userid', u'handle', u'tweet', u'Sentiment', u'Dataset'], dtype='object')"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#here I removve all tweets by the user @Volusion\n",
      "\n",
      "df = df[df['handle'] != 'Volusion']\n",
      "df['Sentiment'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 173,
       "text": [
        "count     193\n",
        "unique      2\n",
        "top       pos\n",
        "freq      171\n",
        "Name: Sentiment, dtype: object"
       ]
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all tweets by mozu as well\n",
      "df = df[df['handle'] != 'Mozu'].reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#evaluate features -> helps us decide which feature selection method to use\n",
      "def evaluate_features(feature_select):\n",
      "    #reading pre-labeled input and splitting into lines\n",
      "    #these are already classified positive/negative sentences from the data set\n",
      "    #it's possible that I should be using the pos/neg information from the dictionary??\n",
      "    posSentences = df[df['Sentiment'] == 'pos'].reset_index()\n",
      "    negSentences = df[df['Sentiment'] == 'neg'].reset_index()\n",
      " \n",
      "    posFeatures = []\n",
      "    negFeatures = []\n",
      "    #breaks up the sentences into lists of individual words (as selected by the input mechanism) \n",
      "    #and appends 'pos' or 'neg' after each list\n",
      "    posrange = len(posSentences)\n",
      "    for i in range(posrange):\n",
      "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", posSentences['tweet'][i].rstrip())\n",
      "        posWords = [feature_select(posWords), 'pos'] #feature_select find all pos words and marks them\n",
      "        posFeatures.append(posWords)\n",
      "    negrange = len(negSentences)\n",
      "    for i in range(negrange):\n",
      "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", negSentences['tweet'][i].rstrip())\n",
      "        negWords = [feature_select(negWords), 'neg']\n",
      "        negFeatures.append(negWords)\n",
      "    \n",
      "    #selects 3/4 of the features to be used for training and 1/4 to be used for testing\n",
      "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
      "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
      "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
      "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
      "\n",
      "    \n",
      "    #trains a Naive Bayes Classifier\n",
      "    classifier = NaiveBayesClassifier.train(trainFeatures)\t\n",
      "\n",
      "    #initiates referenceSets and testSets\n",
      "    referenceSets = collections.defaultdict(set)\n",
      "    testSets = collections.defaultdict(set)\t\n",
      "\n",
      "    #puts correctly labeled sentences in referenceSets and the predictively labeled version in testsets\n",
      "    for i, (features, label) in enumerate(testFeatures):\n",
      "        referenceSets[label].add(i)\n",
      "        predicted = classifier.classify(features)\n",
      "        testSets[predicted].add(i)\t\n",
      "\n",
      "    #prints metrics to show how well the feature selection did\n",
      "    print 'train on %d instances, test on %d instances' % (len(trainFeatures), len(testFeatures))\n",
      "    print 'accuracy:', nltk.classify.util.accuracy(classifier, testFeatures)\n",
      "    print 'pos precision:', nltk.metrics.precision(referenceSets['pos'], testSets['pos'])\n",
      "    print 'pos recall:', nltk.metrics.recall(referenceSets['pos'], testSets['pos'])\n",
      "    print 'neg precision:', nltk.metrics.precision(referenceSets['neg'], testSets['neg'])\n",
      "    print 'neg recall:', nltk.metrics.recall(referenceSets['neg'], testSets['neg'])\n",
      "    classifier.show_most_informative_features(10)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#creates a feature selection mechanism that uses all words\n",
      "def make_full_dict(words):\n",
      "    return dict([(word, True) for word in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tries using all words as the feature selection mechanism\n",
      "print 'using all words as features'\n",
      "evaluate_features(make_full_dict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using all words as features\n",
        "train on 144 instances, test on 49 instances"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy: 0.775510204082\n",
        "pos precision: 0.970588235294\n",
        "pos recall: 0.767441860465\n",
        "neg precision: 0.333333333333\n",
        "neg recall: 0.833333333333\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                    soon = True              neg : pos    =     12.6 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                Volusion = None              pos : neg    =      8.9 : 1.0\n",
        "                  You're = True              neg : pos    =      7.6 : 1.0\n",
        "                      am = True              neg : pos    =      7.6 : 1.0\n",
        "                      if = True              neg : pos    =      7.6 : 1.0\n",
        "                    tech = True              neg : pos    =      7.6 : 1.0\n",
        "                  having = True              neg : pos    =      7.6 : 1.0\n",
        "                    like = True              neg : pos    =      7.6 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#scores words based on chi-squared test to show information gain (http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/)\n",
      "def create_word_scores():\n",
      "    posWords = []\n",
      "    negWords = []    \n",
      "    posSentences = df[df['Sentiment'] == 'pos'].reset_index()\n",
      "    negSentences = df[df['Sentiment'] == 'neg'].reset_index()    \n",
      "    #creates lists of all positive and negative words\n",
      "    posrange = len(posSentences)\n",
      "    for i in range(posrange):\n",
      "        posWord = re.findall(r\"[\\w']+|[.,!?;]\", posSentences['tweet'][i].rstrip())\n",
      "        posWords.append(posWord)        \n",
      "    negrange = len(negSentences)\n",
      "    for i in range(negrange):\n",
      "        negWord = re.findall(r\"[\\w']+|[.,!?;]\", negSentences['tweet'][i].rstrip())\n",
      "        negWords.append(negWord)\n",
      "    posWords = list(itertools.chain(*posWords))\n",
      "    negWords = list(itertools.chain(*negWords))\n",
      "\n",
      "    #build frequency distibution of all words and then frequency distributions of words within positive and negative labels\n",
      "    word_fd = FreqDist()\n",
      "    cond_word_fd = ConditionalFreqDist()\n",
      "    for word in posWords:\n",
      "        word_fd[word.lower()] += 1\n",
      "        cond_word_fd['pos'][word.lower()] += 1\n",
      "    for word in negWords:\n",
      "        word_fd[word.lower()] += 1\n",
      "        cond_word_fd['neg'][word.lower()] += 1\n",
      "\n",
      "    #finds the number of positive and negative words, as well as the total number of words\n",
      "    pos_word_count = cond_word_fd['pos'].N()\n",
      "    neg_word_count = cond_word_fd['neg'].N()\n",
      "    total_word_count = pos_word_count + neg_word_count\n",
      "\n",
      "    #builds dictionary of word scores based on chi-squared test\n",
      "    word_scores = {}\n",
      "    for word, freq in word_fd.iteritems():\n",
      "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)\n",
      "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)\n",
      "        word_scores[word] = pos_score + neg_score\n",
      "\n",
      "    return word_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#finds word scores\n",
      "word_scores = create_word_scores()\n",
      "\n",
      "#finds the best 'number' words based on word scores\n",
      "def find_best_words(word_scores, number):\n",
      "    best_vals = sorted(word_scores.iteritems(), key=lambda (w, s): s, reverse=True)[:number]\n",
      "    best_words = set([w for w, s in best_vals])\n",
      "    return best_words\n",
      "\n",
      "#creates feature selection mechanism that only uses best words\n",
      "def best_word_features(words):\n",
      "    return dict([(word, True) for word in words if word in best_words])\n",
      "\n",
      "#numbers of features to select\n",
      "numbers_to_test = [10, 100, 1000, 10000, 15000]\n",
      "#tries the best_word_features mechanism with each of the numbers_to_test of features\n",
      "for num in numbers_to_test:\n",
      "    print 'evaluating best %d word features' % (num)\n",
      "    best_words = find_best_words(word_scores, num)\n",
      "    evaluate_features(best_word_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "evaluating best 10 word features\n",
        "train on 144 instances, test on 49 instances\n",
        "accuracy: 0.897959183673\n",
        "pos precision: 0.931818181818\n",
        "pos recall: 0.953488372093\n",
        "neg precision: 0.6\n",
        "neg recall: 0.5\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                      is = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = None              pos : neg    =      1.6 : 1.0\n",
        "                    down = None              pos : neg    =      1.6 : 1.0\n",
        "                       ? = None              pos : neg    =      1.4 : 1.0\n",
        "                     not = None              pos : neg    =      1.3 : 1.0\n",
        "                 service = None              pos : neg    =      1.2 : 1.0\n",
        "                   after = None              pos : neg    =      1.2 : 1.0\n",
        "              constantly = None              pos : neg    =      1.2 : 1.0\n",
        "evaluating best 100 word features\n",
        "train on 144 instances, test on 49 instances\n",
        "accuracy: 0.938775510204\n",
        "pos precision: 0.954545454545\n",
        "pos recall: 0.976744186047\n",
        "neg precision: 0.8\n",
        "neg recall: 0.666666666667\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                    just = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n",
        "                    like = True              neg : pos    =      7.6 : 1.0\n",
        "                shopping = True              neg : pos    =      7.6 : 1.0\n",
        "                      an = True              neg : pos    =      5.4 : 1.0\n",
        "                     are = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = None              pos : neg    =      1.6 : 1.0\n",
        "evaluating best 1000 word features\n",
        "train on 144 instances, test on 49 instances"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy: 0.673469387755\n",
        "pos precision: 0.965517241379\n",
        "pos recall: 0.651162790698\n",
        "neg precision: 0.25\n",
        "neg recall: 0.833333333333\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                    soon = True              neg : pos    =     12.6 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                      am = True              neg : pos    =      7.6 : 1.0\n",
        "                      if = True              neg : pos    =      7.6 : 1.0\n",
        "                   reach = True              neg : pos    =      7.6 : 1.0\n",
        "                business = True              neg : pos    =      7.6 : 1.0\n",
        "                    days = True              neg : pos    =      7.6 : 1.0\n",
        "                    tech = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n",
        "evaluating best 10000 word features\n",
        "train on 144 instances, test on 49 instances"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy: 0.673469387755\n",
        "pos precision: 0.965517241379\n",
        "pos recall: 0.651162790698\n",
        "neg precision: 0.25\n",
        "neg recall: 0.833333333333\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                    soon = True              neg : pos    =     12.6 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                      am = True              neg : pos    =      7.6 : 1.0\n",
        "                      if = True              neg : pos    =      7.6 : 1.0\n",
        "                   reach = True              neg : pos    =      7.6 : 1.0\n",
        "                business = True              neg : pos    =      7.6 : 1.0\n",
        "                    days = True              neg : pos    =      7.6 : 1.0\n",
        "                    tech = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n",
        "evaluating best 15000 word features\n",
        "train on 144 instances, test on 49 instances"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy: 0.673469387755\n",
        "pos precision: 0.965517241379\n",
        "pos recall: 0.651162790698\n",
        "neg precision: 0.25\n",
        "neg recall: 0.833333333333\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                    soon = True              neg : pos    =     12.6 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                      am = True              neg : pos    =      7.6 : 1.0\n",
        "                      if = True              neg : pos    =      7.6 : 1.0\n",
        "                   reach = True              neg : pos    =      7.6 : 1.0\n",
        "                business = True              neg : pos    =      7.6 : 1.0\n",
        "                    days = True              neg : pos    =      7.6 : 1.0\n",
        "                    tech = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plot of accuracy against features\n",
      "\n",
      "x = [10, 100, 1000, 10000, 15000, 15001]\n",
      "y = [0.897959, 0.93877, 0.673469, 0.673469387755, 0.673469387755, 0.775510204082]\n",
      "plt.plot(x, y)\n",
      "plt.ylabel('Accuracy')\n",
      "plt.xlabel('# of Features')\n",
      "plt.title('Accuracy as a Function of Features')\n",
      "savefig('Feature-accuracy.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEZCAYAAACw69OmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x16630780>"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'evaluating best %d word features' % (100)\n",
      "best_words = find_best_words(word_scores, 100)\n",
      "evaluate_features(best_word_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "evaluating best 100 word features\n",
        "train on 144 instances, test on 49 instances\n",
        "accuracy: 0.938775510204\n",
        "pos precision: 0.954545454545\n",
        "pos recall: 0.976744186047\n",
        "neg precision: 0.8\n",
        "neg recall: 0.666666666667\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                    just = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n",
        "                    like = True              neg : pos    =      7.6 : 1.0\n",
        "                shopping = True              neg : pos    =      7.6 : 1.0\n",
        "                      an = True              neg : pos    =      5.4 : 1.0\n",
        "                     are = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = None              pos : neg    =      1.6 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now to classify the dataset\n",
      "\n",
      "#pos/neg classified items\n",
      "posSentences = df[df['Sentiment'] == 'pos'].reset_index()\n",
      "negSentences = df[df['Sentiment'] == 'neg'].reset_index()\n",
      " \n",
      "posFeatures = []\n",
      "negFeatures = []\n",
      "\n",
      "#breaks up the sentences into lists of individual words (as selected by the input mechanism) \n",
      "#and appends 'pos' or 'neg' after each list\n",
      "posrange = len(posSentences)\n",
      "for i in range(posrange):\n",
      "    posWords = re.findall(r\"[\\w']+|[.,!?;]\", posSentences['tweet'][i].rstrip())\n",
      "    posWords = [best_word_features(posWords), 'pos'] #feature_select find all pos words and marks them\n",
      "    posFeatures.append(posWords)\n",
      "negrange = len(negSentences)\n",
      "for i in range(negrange):\n",
      "    negWords = re.findall(r\"[\\w']+|[.,!?;]\", negSentences['tweet'][i].rstrip())\n",
      "    negWords = [best_word_features(negWords), 'neg']\n",
      "    negFeatures.append(negWords)\n",
      "    \n",
      "#selects 3/4 of the features to be used for training and 1/4 to be used for testing\n",
      "posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
      "negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
      "trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
      "testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
      "        \n",
      "#trains a Naive Bayes Classifier\n",
      "classifier = NaiveBayesClassifier.train(trainFeatures)\t\n",
      "\n",
      "#initiates referenceSets and testSets\n",
      "referenceSets = collections.defaultdict(set)\n",
      "testSets = collections.defaultdict(set)\t\n",
      "\n",
      "#puts correctly labeled sentences in referenceSets and the predictively labeled version in testsets\n",
      "for i, (features, label) in enumerate(testFeatures):\n",
      "    referenceSets[label].add(i)\n",
      "    predicted = classifier.classify(features)\n",
      "    testSets[predicted].add(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prints metrics to show how well the feature selection did\n",
      "print 'train on %d instances, test on %d instances' % (len(trainFeatures), len(testFeatures))\n",
      "print 'accuracy:', nltk.classify.util.accuracy(classifier, testFeatures)\n",
      "print 'pos precision:', nltk.metrics.precision(referenceSets['pos'], testSets['pos'])\n",
      "print 'pos recall:', nltk.metrics.recall(referenceSets['pos'], testSets['pos'])\n",
      "print 'neg precision:', nltk.metrics.precision(referenceSets['neg'], testSets['neg'])\n",
      "print 'neg recall:', nltk.metrics.recall(referenceSets['neg'], testSets['neg'])\n",
      "classifier.show_most_informative_features(10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train on 144 instances, test on 49 instances\n",
        "accuracy: 0.938775510204\n",
        "pos precision: 0.954545454545\n",
        "pos recall: 0.976744186047\n",
        "neg precision: 0.8\n",
        "neg recall: 0.666666666667\n",
        "Most Informative Features\n",
        "                 service = True              neg : pos    =     17.7 : 1.0\n",
        "                       ? = True              neg : pos    =     11.9 : 1.0\n",
        "                    just = True              neg : pos    =      7.6 : 1.0\n",
        "                     too = True              neg : pos    =      7.6 : 1.0\n",
        "                    like = True              neg : pos    =      7.6 : 1.0\n",
        "                shopping = True              neg : pos    =      7.6 : 1.0\n",
        "                      an = True              neg : pos    =      5.4 : 1.0\n",
        "                     are = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = True              neg : pos    =      5.4 : 1.0\n",
        "                      is = None              pos : neg    =      1.6 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create the dataset to test on\n",
      "\n",
      "trainFeatures = posFeatures + negFeatures\n",
      "\n",
      "MyTest = []\n",
      "for x in range(len(df)):\n",
      "    #print x\n",
      "    mywords = re.findall(r\"[\\w']+|[.,!?;]\", df['tweet'][x].rstrip())\n",
      "    if df['Sentiment'][x] == 'pos':\n",
      "        mywords = [best_word_features(mywords), 'pos'] \n",
      "        MyTest.append(mywords)\n",
      "    if df['Sentiment'][x] == 'neg':\n",
      "        mywords = [best_word_features(mywords), 'neg'] \n",
      "        MyTest.append(mywords)\n",
      "    elif type(df['Sentiment'][x]) == float:\n",
      "        mywords = [best_word_features(mywords), ''] \n",
      "        MyTest.append(mywords)\n",
      "        \n",
      "classifier = NaiveBayesClassifier.train(trainFeatures)\n",
      "\n",
      "#initiates referenceSets and testSets\n",
      "referenceSets = collections.defaultdict(set)\n",
      "testSets = collections.defaultdict(set)\t\n",
      "\n",
      "df['Probability']= ''\n",
      "df['Pred_Sent']= ''\n",
      "df['assigned_sent'] = ''\n",
      "#puts correctly labeled sentences in referenceSets and the predictively labeled version in testsets\n",
      "for i, (features, label) in enumerate(MyTest):\n",
      "    referenceSets[label].add(i)\n",
      "    predicted = classifier.classify(features)\n",
      "    #print predicted\n",
      "    distr = classifier.prob_classify(features)\n",
      "    #print(\"%s: %f\" % (predicted, distr.prob(predicted)))\n",
      "    df['Probability'][i] = distr.prob(predicted)\n",
      "    if distr.prob(predicted) < 0.88:\n",
      "        df['assigned_sent'][i] = 'neu'\n",
      "        df['Pred_Sent'][i] = predicted\n",
      "    else:\n",
      "        df['assigned_sent'][i] = predicted\n",
      "        df['Pred_Sent'][i] = predicted\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "totalclassed = {'neu':len(df[df['assigned_sent'] == 'neu']), 'pos':len(df[df['assigned_sent'] == 'pos']) , 'neg':len(df[df['assigned_sent'] == 'neg'])}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "totalclassed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 190,
       "text": [
        "{'neg': 698, 'neu': 1661, 'pos': 5216}"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 191,
       "text": [
        "7575"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_csv('Volusion_classed.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "volusion = df[(df['Dataset'] == 'Volusion')]\n",
      "len(volusion[volusion['assigned_sent']=='pos']), len(volusion[volusion['assigned_sent']=='neg']), len(volusion[volusion['assigned_sent']=='neu']), "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 219,
       "text": [
        "(175, 41, 62)"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(volusion[volusion['assigned_sent'] == 'neg']['handle']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 228,
       "text": [
        "34"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(volusion), len(mozu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 207,
       "text": [
        "(278, 44)"
       ]
      }
     ],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mozu = df[(df['Dataset'] == 'Mozu')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shopify = df[(df['Dataset'] == 'Shopify')]\n",
      "print len(shopify[shopify['assigned_sent'] == 'pos']), len(shopify[shopify['assigned_sent'] == 'neg']), len(shopify[shopify['assigned_sent'] == 'neu'])\n",
      "print len(shopify)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5020 657 1576\n",
        "7253\n"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mydf = pd.concat([volusion, mozu])\n",
      "len(mydf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 220,
       "text": [
        "322"
       ]
      }
     ],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'pos:%d , neg:%d, neu:%d' % (len(mydf[mydf['assigned_sent'] == 'pos']), len(mydf[mydf['assigned_sent'] == 'neg']), len(mydf[mydf['assigned_sent'] == 'neu']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 201,
       "text": [
        "'pos:196 , neg:41, neu:85'"
       ]
      }
     ],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "volusion_neg = mydf[mydf['assigned_sent'] == 'neg']\n",
      "\n",
      "volusion_atmentions = mydf[mydf['tweet'].str.contains(r'@Volusion')]\n",
      "#for x in \n",
      "#of the negative \n",
      "#word_scores\n",
      "\n",
      "#most_common"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'neu:' + str(len(volusion_atmentions[volusion_atmentions['assigned_sent']=='neu']))\n",
      "print 'neg:' + str(len(volusion_atmentions[volusion_atmentions['assigned_sent']=='neg']))\n",
      "print 'pos:' + str(len(volusion_atmentions[volusion_atmentions['assigned_sent']=='pos']))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "neu:26\n",
        "neg:30\n",
        "pos:55\n"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mozu_atmentions = mydf[mydf['tweet'].str.contains(r'@MozuCommerce')]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'neu:' + str(len(mozu_atmentions[mozu_atmentions['assigned_sent']=='neu']))\n",
      "print 'neg:' + str(len(mozu_atmentions[mozu_atmentions['assigned_sent']=='neg']))\n",
      "print 'pos:' + str(len(mozu_atmentions[mozu_atmentions['assigned_sent']=='pos']))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "neu:25\n",
        "neg:0\n",
        "pos:17\n"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#just look at at_mentionsl\n",
      "for x in volusion_atmentions['tweet']:\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@Volusion is the WORST is pure SH*T. Shopping carts constantly crash. POOR customer service. #YOUSUCK\n",
        "RT @flyingorange: Work in Progress for Appleseed Primitives: Custom @Volusion template for #ecommerce website #webdesign #html5 #rwd http:/\u2026\n",
        "RT @TweetPopSocial: #FacebookFans vs. #TwitterFollowers \u2013 Which Are Better? http://t.co/UO9r1IzWFc via @Volusion\n",
        "Attend @Volusion's Feb 10th webinar for an intro of their robust, yet simple, platform. Signup @ http://t.co/qoIlFvkP08 #ecommerce #startups\n",
        "RT @Xconomy: Texas Roundup: @Volusion, @seatsmart, @XenexDisinfect, Decisio, @BioHouston, @AveXisInc | http://t.co/2fQdc77HTy\n",
        "Our @Volusion integration keeps your quantities synced between your warehouse and your store. Learn more: http://t.co/m3JHCcInMh #ecommerce\n",
        "RT @alkerton: When I joined @Shopify, we were at 12K stores, @BigCommerce was around 30K, and @Volusion at 40K. Today, we're bigger than bo\u2026\n",
        "@Volusion  We are new to volusion.  Is this common for our web sites to go down?  This is not good for business.  I am worried.\n",
        "How to Grow an Ecommerce Business - Volusion via @Volusion http://t.co/R1ZVhs8YYj\n",
        "Softwaremaker @Volusion announced completion of $55M round of equity financing + plans to hire 150 workers this year. http://t.co/XTKuio18Yr\n",
        "The Kid Brilliance Times is out! http://t.co/MVanQmCreZ Stories via @calgaryzoo @Volusion @AnnVoskamp\n",
        "@Volusion Do we have an estimate of when this issue will be resolved?  Is it a server issue?\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY7rc28 http://t.co/ZYbH\u2026\n",
        "@Volusion Thanks.  Much appreciated.\n",
        "@Volusion integration: How it's more than sales order synchronisation http://t.co/7xyMYU6St3 #ecommerce\n",
        "@Volusion How do I contact Volusion to let you know our site is down too?\n",
        "@Volusion noooo server not responding @sobareboudoir is not reloading \ud83d\ude15 #fix #everything #soon\n",
        "From @Volusion: personal connection keeps startup vibe alive during rapid #growth http://t.co/crmMlcagzs #FutureofWork\n",
        "@Volusion @EdmundSLee @sfemerick Five minutes after posting this, your service went down and you were totally unresponsive to inquiries\u2026\n",
        "The Kid Brilliance Times is out! http://t.co/MVanQmkPQp Stories via @shibleysmiles @Volusion @AmyBlevins\n",
        "Thanks #TopNewFollowers @Volusion @GrowthHackerAm @KarolPokojowczy Happy to connect :)\n",
        "RT @flyingorange: Work in Progress for Appleseed Primitives: Custom @Volusion template for #ecommerce website #webdesign #html5 #rwd http:/\u2026\n",
        "@Volusion http://t.co/kXSUXCqlSY is off line\n",
        "RT @Volusion: FREE #ValentinesDay graphics: Merchants, use these free images to help promote your offers http://t.co/tZyBH00gfy http://t.co\u2026\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY7rc28 http://t.co/ZYbH\u2026\n",
        "@Volusion It doesn't look like just some websites...Volusion.com is currently down!!\n",
        "@Volusion Get your SH*T together! You're system is constantly crashing. What the hell is wrong with you!?\n",
        "RT @Volusion: 3 tips to build a user-friendly site nav menu to increase conversions &amp; revenue: http://t.co/Cf8Fmk1QRY by @Nextopia http://t\u2026\n",
        "#Ecommerce software provider @Volusion just announced a really big #funding round. http://t.co/vLgnqj51EY\n",
        "Anyone thinking of signing up for @Volusion -- DON'T. Unreliable shopping cart. Constantly crashing. Poor customer service. #DISASTER\n",
        "RT @Volusion: LAST CHANCE! Enter to win 1yr of FREE website hosting + other prizes valued at $25k. Enter: http://t.co/IIkB0drYdG http://t.c\u2026\n",
        "@Volusion When can we expect full functionality? I hope ASAP! We're going nuts over here!\n",
        "When I joined @Shopify, we were at 12K stores, @BigCommerce was around 30K, and @Volusion at 40K. Today, we're bigger than both combined.\n",
        "The Wida Leicester Weekly is out! http://t.co/PaA8jdWiiF Stories via @Volusion\n",
        "@Volusion Feeling dejected with your support. Got a 5th grader response on a ticket after 5 days and still no resolution! #UnhappyCustomer\n",
        "@Volusion looks all set now. thx\n",
        "@Volusion Thanks for keeping us posted! @IETrainings appreciates it!\n",
        "@Volusion had to share this on my @RebelMouse. Thank you! http://t.co/PpiEgz0w48\n",
        "@Volusion Is Volusion having problems right now? My company's website won't load and I can't load http://t.co/C3y94PPn4t either.\n",
        "RT @Volusion: Keep up the great work, #entrepreneurs! http://t.co/beOvSjC34q\n",
        "Congrats @Volusion merchant Drain Strain for scoring a deal with @robertherjavec in @SharkTankABC #sharktanknation #ecommerce #DrainStrain\n",
        "RT @ZakStamborIR: #Ecommerce software provider @Volusion raises $55 million. http://t.co/vLgnqj51EY\n",
        "@Volusion Thanks for keeping us posted!\n",
        "Trust @Volusion. Personal connection keeps startup vibe alive during growth http://t.co/VjiIfrGiaH #FutureofWork\n",
        "RT @Volusion: 3 tips to build a user-friendly site nav menu to increase conversions &amp; revenue: http://t.co/Cf8Fmk1QRY by @Nextopia http://t\u2026\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/HnbXcwwof4 http://t.c\u2026\n",
        "@Volusion Sincere apologies my ass. You guys are a complete #DUMPSTERFIRE. Who do you hire to do I.T.? A bunch of high school stoners?\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY7rc28 http://t.co/ZYbH\u2026\n",
        "Thanks, Grant, have a wonderful weekend! @gwickes @Volusion @alisoncdiana @ScottVann\n",
        "RT @Volusion: Learn how to start selling online by coming to a 30min presentation on the Volusion platform on 2/10: http://t.co/FKU6AwIT2Z \u2026\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY7rc28 http://t.co/ZYbH\u2026\n",
        "RT @Volusion: 3 tips to build a user-friendly site nav menu to increase conversions &amp; revenue: http://t.co/Cf8Fmk1QRY by @Nextopia http://t\u2026\n",
        "Hey @Volusion - Our sites are down too.  cc: @thetruthinred\n",
        "RT @Volusion: Need a fresh design for your online store? Examples: http://t.co/TisDOLjotX Save 15-20% off select designs! Go here: http://t\u2026\n",
        "@Volusion Service is down. Can't login to MyVolusion?\n",
        "@Volusion BTW, if I click on the picture or ow.ly link, I can NOT get to the contest entry.  No bueno.\n",
        "@Volusion all my sites are down. Any service updates?\n",
        "Great week of RTs and interaction. Thx! #FF @Volusion @lauriemccabe @alisoncdiana @ScottVann\n",
        "RT @angelashah: Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor https://t.co/xAhrOOTv0R\u2026\n",
        "RT @angelashah: Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor http://t.co/8JDgI5iNXo \u2026\n",
        "Hey @nathanjoynt is there any way I can send you an email regarding @Volusion guest blogging? Thanks!\n",
        "January Top - List of Best #e-Commerce Articles http://t.co/hB8aiNFMDM\n",
        "@Shopify @Volusion @PrestaShop @Bigcommerce http://t.co/CVkZ1CjUUO\n",
        "RT @Mpowerwebsol @Volusion launched an #ecommerce #website for Pear Tree Soaps - handmade bath &amp; body care products. http://t.co/gkmmr6CWX8\n",
        "Texas Roundup: @Volusion, @seatsmart, @XenexDisinfect, Decisio, @BioHouston, @AveXisInc | http://t.co/2fQdc77HTy\n",
        "It looks like Bigcommerce's CEO approval rating is 83 vs. @Volusion's 100 Vote: http://t.co/BtnrOF4z3M\n",
        "For the man that has some face bits to shave - REVIEW - Beard Bars Cream Shave @Volusion http://t.co/S37lGN0jNU\n",
        "RT @Volusion: 3 tips to build a user-friendly site nav menu to increase conversions &amp; revenue: http://t.co/Cf8Fmk1QRY by @Nextopia http://t\u2026\n",
        "RT @ZakStamborIR: #Ecommerce software provider @Volusion just announced a really big #funding round. http://t.co/vLgnqj51EY\n",
        "From @Volusion: personal connection keeps startup vibe alive during rapid #growth http://t.co/crmMlcagzs #FutureofWork\n",
        "RT @Volusion: Valentine's Day gifts from Volusion stores: http://t.co/A1iLXhi4Qf w/ @7seaspearls @STITCHchicago @artwareeditions http://t.c\u2026\n",
        "Hey there, @Volusion!  See you at the Miami Boat Show!\n",
        "@Volusion are your servers down? Mine and a friends sites aren't loading.\n",
        "@Volusion You're welcome, happy to share nice images :)\n",
        "@Volusion All of our sites are down and cannot reach tech support.\n",
        "RT @Volusion: Unique #ValentinesDay gifts from #Volusion merchants for your special guy: http://t.co/QyD2b8W4eN w/ @lotfautographs @drinkup\u2026\n",
        "RT @angelashah: Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor http://t.co/8JDgI5iNXo \u2026\n",
        "RT @Volusion: @gwickes @lauriemccabe Great read! We share our 2015 ecommerce predictions for SMBs here: http://t.co/Mv790MZmdU.\n",
        "First real day working with @Volusion ... kind of like the Seattle play last night:( #beastmode not! Site down twice! http://t.co/voFBiVYo56\n",
        "@Volusion we are back online!\n",
        "The Wrexham FM Daily is out! http://t.co/SYVLz3BtTK Stories via @Volusion\n",
        "Thanks @yandex @itBit @robocoin @Volusion for the bounty!!!\n",
        "RT @angelashah: Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor http://t.co/8JDgI5iNXo \u2026\n",
        "It looks like Ignify's CEO approval rating is 75 vs. @Volusion's 100 Vote: http://t.co/NQZad0h2QR\n",
        "RT @peter_sheldon: Q: What's fueling the growth of the top 3 mid-market eComm platforms (@Shopify, @Volusion, @Bigcommerce)? A: a whopping \u2026\n",
        "@Volusion servers are down. I should rely on self-hosted solutions instead. Their bandwidth costs are also too damn high.\n",
        "@Volusion @drewsanocki Thanks for the mentions. Happy to answer any questions here or back at: http://t.co/cXEJ44awRs\n",
        "RT @Volusion: Unique #ValentinesDay gifts from #Volusion merchants for your special guy: http://t.co/QyD2b8W4eN w/ @windydayweather @STITCH\u2026\n",
        "Q: What's fueling the growth of the top 3 mid-market eComm platforms (@Shopify, @Volusion, @Bigcommerce)? A: a whopping $337m of VC funding\n",
        "RT @peter_sheldon: Q: What's fueling the growth of the top 3 mid-market eComm platforms (@Shopify, @Volusion, @Bigcommerce)? A: a whopping \u2026\n",
        "RT @Volusion: FREE #ValentinesDay graphics: Merchants, use these free images to help promote your offers http://t.co/tZyBGZ9sRm http://t.co\u2026\n",
        "@Volusion Who is your CEO? Is he/she incompetent or just an idiot? One or two system crashes a YEAR might be acceptable. But not per MONTH.\n",
        "Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor https://t.co/xAhrOOTv0R via @xconomy TX\n",
        "Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor http://t.co/8JDgI5iNXo via @xconomy TX\n",
        "@Volusion Create the buzz you need to be a success using this new tool http://t.co/GRO2GcESXN\n",
        "#Austin\u2019s @Volusion Raises $55M to Further Develop E-Retail Software http://t.co/qWE20yuqKx #technology\n",
        "#Ecommerce software provider @Volusion raises $55 million. http://t.co/vLgnqj51EY\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY80o1K http://t.co/SnXs\u2026\n",
        "From @Volusion: personal connection keeps startup vibe alive during rapid #growth http://t.co/0bSA5fdDr8 #FutureofWork\n",
        "@alkerton @Shopify @Bigcommerce @Volusion congrats!\n",
        "Menswear Today is out! http://t.co/3xT8RYlJGm Stories via @AustinPriceless @Volusion\n",
        ".@Volusion is looking for a User Experience Designer in Austin (U.S.A.). More info: http://t.co/AYRP91qMs0 #jobs\n",
        "@Volusion integration: How it's more than sales order synchronisation http://t.co/NBnMlGkn5V #ecommerce\n",
        "RT @Volusion: #ValentinesDay is the 2nd biggest selling holiday of the year. Don't miss these tips: http://t.co/pStcY7rc28 http://t.co/ZYbH\u2026\n",
        "@Volusion it got back up pretty quick for me. Thanks for being speedy!\n",
        "RT @VentureBeat: .@Volusion raises $55M to fund its e-commerce shopping cart http://t.co/Ss4Zppx7Ee\n",
        "RT @angelashah: Texas Roundup: @Volusion @seatsmart @XenexDisinfect Decisio @BioHouston @AveXisInc @Localeur @Favor https://t.co/xAhrOOTv0R\u2026\n",
        "Trust @Volusion. Personal connection keeps startup vibe alive during growth http://t.co/VjiIfsx5yT #FutureofWork\n",
        "@Volusion your status updates are not really updates are they?\n",
        "Work in Progress for Appleseed Primitives: Custom @Volusion template for #ecommerce website #webdesign #html5 #rwd http://t.co/GLk1SiMBWo\n",
        "@Volusion Thank you for the speedy update! We were scrambling to find a way to reach your team to correct our downtime.\n",
        "@Volusion 20 #eCommerce Website A/B Testing Ideas To Boost Online Sales http://t.co/JVbEVCi3ki\n"
       ]
      }
     ],
     "prompt_number": 165
    }
   ],
   "metadata": {}
  }
 ]
}